{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch.utils.data as utils\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PitchClass =[  # The order of these files need to be the same\n",
    "    'Pitched_Amplitude.csv',\n",
    "    'Pitched_Frequency.csv',\n",
    "    'Pitched_SensoryDissonance.csv',\n",
    "    'Pitched_SpectralEntropy.csv'   \n",
    "]\n",
    "TimbralClass = [\n",
    "   'Timbral_Amplitude.csv',\n",
    "    'Timbral_Frequency.csv',\n",
    "    'Timbral_SensoryDissonance.csv',\n",
    "    'Timbral_SpectralEntropy.csv'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.21900e-02 4.40000e+02 0.00000e+00 0.00000e+00]\n",
      " [5.56931e-03 2.10283e+04 0.00000e+00 5.32513e+01]\n",
      " [5.93050e-03 2.10699e+04 0.00000e+00 8.66560e+01]\n",
      " ...\n",
      " [3.53296e-01 4.40265e+02 5.82047e+01 2.09007e+00]\n",
      " [1.49441e-01 4.38142e+02 3.16227e+00 4.08216e+00]\n",
      " [2.39323e-02 4.38142e+02 0.00000e+00 7.80765e+01]]\n"
     ]
    }
   ],
   "source": [
    "def LoadCSVs(CSVFileArray):\n",
    "    OutPutArray=[]\n",
    "    for csvFile in CSVFileArray:\n",
    "        TempArray=[]\n",
    "        with open(csvFile, newline='') as TheFile:\n",
    "            fileReader = csv.reader(TheFile, delimiter=',' )\n",
    "            for row in fileReader:\n",
    "                TempArray.append(np.float64(row[0]))\n",
    "        OutPutArray.append(TempArray)\n",
    "    return np.array(OutPutArray).T\n",
    "print(LoadCSVs(TimbralClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimbralData = LoadCSVs(TimbralClass)\n",
    "PitchData = LoadCSVs(PitchClass)\n",
    "\n",
    "ShortesLength = min(len(TimbralData),len(PitchData))\n",
    "TimbralData=TimbralData[:ShortesLength]\n",
    "PitchData=PitchData[:ShortesLength]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompleteInputData = np.concatenate((TimbralData, PitchData), axis=0)\n",
    "classes=['TimbralData','PitchData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.zeros(1),torch.ones(5))).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1346, 4]) torch.Size([1346])\n"
     ]
    }
   ],
   "source": [
    "tensor_x = torch.stack([torch.Tensor(i) for i in CompleteInputData]) # transform to torch tensors\n",
    "tensor_y = torch.cat((torch.zeros(len(TimbralData)),torch.ones(len(PitchData)))).long()\n",
    "tensor_y.long()\n",
    "print(tensor_x.shape,tensor_y.shape)\n",
    "\n",
    "my_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "\n",
    "train_size = int(0.8 * len(my_dataset))\n",
    "test_size = len(my_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(my_dataset, [train_size, test_size])\n",
    "my_trainloader = utils.DataLoader(train_dataset,batch_size=16,shuffle=True) \n",
    "my_testloader = utils.DataLoader(test_dataset,batch_size=16,shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size (number of variables) should be: 4\n"
     ]
    }
   ],
   "source": [
    "InputSize = tensor_x.shape[1]\n",
    "NumberOfClasses = len(classes)\n",
    "\n",
    "print(\"Input size (number of variables) should be:\",InputSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNetwork,self).__init__()\n",
    "        \n",
    "        self.hiddenLayerSize=64\n",
    "        self.hiddenLayerSize2=10\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(InputSize)\n",
    "        self.fc0 = nn.Linear(InputSize, self.hiddenLayerSize)\n",
    "        self.fc1 = nn.Linear(self.hiddenLayerSize,self.hiddenLayerSize2) \n",
    "        self.fc2 = nn.Linear(self.hiddenLayerSize2,NumberOfClasses) \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''   \n",
    "        x = self.bn0(x)\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNetwork(\n",
      "  (bn0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc0): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (fc1): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleNetwork(\n",
       "  (bn0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc0): Linear(in_features=4, out_features=64, bias=True)\n",
       "  (fc1): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleNetwork()\n",
    "print(model)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter=0;\n",
    "LossOverEpoch=[]\n",
    "EvalLoss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunForNumEpochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ImageCalc/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5770346105975264   1000  0.5549151213730082\n",
      "Training loss: 0.5708417230669189   1001  0.547576087362626\n",
      "Training loss: 0.5731095697949914   1002  0.5487956842955422\n",
      "Training loss: 0.5700101760380408   1003  0.566426694393158\n",
      "Training loss: 0.5711533146746018   1004  0.553884786718032\n",
      "Training loss: 0.5724672872354003   1005  0.5501729302546557\n",
      "Training loss: 0.5742421904030968   1006  0.5542891656651217\n",
      "Training loss: 0.5675658636233386   1007  0.5547427114318398\n",
      "Training loss: 0.5641825400731143   1008  0.546841479399625\n",
      "Training loss: 0.5724191556082052   1009  0.5484809963142171\n",
      "Training loss: 0.572867214679718   1010  0.5526275739950293\n",
      "Training loss: 0.5635797078995144   1011  0.5588614221881417\n",
      "Training loss: 0.5550644502920263   1012  0.5522581181105446\n",
      "Training loss: 0.5652862800394788   1013  0.5638207933482002\n",
      "Training loss: 0.5680288177202729   1014  0.5381544407676248\n",
      "Training loss: 0.5664420785272822   1015  0.5682346663054298\n",
      "Training loss: 0.5826414762174382   1016  0.5471916619469138\n",
      "Training loss: 0.5708357402506996   1017  0.5659229755401611\n",
      "Training loss: 0.56611119254547   1018  0.5452109066879048\n",
      "Training loss: 0.5854444959584404   1019  0.5534099182661842\n",
      "Training loss: 0.5650935580625254   1020  0.5481090352815741\n",
      "Training loss: 0.5684117240940824   1021  0.5428604904343101\n",
      "Training loss: 0.5649139911812895   1022  0.5437077564351699\n",
      "Training loss: 0.5781782799783874   1023  0.5520742342752569\n",
      "Training loss: 0.5731562332195395   1024  0.5459312077830819\n",
      "Training loss: 0.5679580323836383   1025  0.5610520401421715\n",
      "Training loss: 0.5699111512478661   1026  0.5498634699512931\n",
      "Training loss: 0.569528188337298   1027  0.593685879426844\n",
      "Training loss: 0.5634482283802593   1028  0.55789195264087\n",
      "Training loss: 0.5650365804048145   1029  0.5513403328026042\n",
      "Training loss: 0.5686217883930487   1030  0.5448881352649015\n",
      "Training loss: 0.5739382286282146   1031  0.5621840427903568\n",
      "Training loss: 0.569583535194397   1032  0.5418134647257188\n",
      "Training loss: 0.5832517449470127   1033  0.5486499667167664\n",
      "Training loss: 0.5780915933496812   1034  0.5491184241631452\n",
      "Training loss: 0.5669471652192228   1035  0.5605511630282682\n",
      "Training loss: 0.5731829777359962   1036  0.5553988235838273\n",
      "Training loss: 0.5619669609210071   1037  0.5520686261794147\n",
      "Training loss: 0.5676975587711615   1038  0.5663910806179047\n",
      "Training loss: 0.559887966250672   1039  0.5832655359716976\n",
      "Training loss: 0.5671508136917564   1040  0.5600974892868715\n",
      "Training loss: 0.581830175483928   1041  0.5680048115113202\n",
      "Training loss: 0.5710271114812178   1042  0.5469691402771893\n",
      "Training loss: 0.5611711731728386   1043  0.5414674247012419\n",
      "Training loss: 0.5676079041817609   1044  0.5570791100754457\n",
      "Training loss: 0.5696345821899527   1045  0.5507747828960419\n",
      "Training loss: 0.5698847113286748   1046  0.5630624311811784\n",
      "Training loss: 0.5670635060352438   1047  0.5473060011863708\n",
      "Training loss: 0.5794164547148872   1048  0.5410627775332507\n",
      "Training loss: 0.5597640043672394   1049  0.5467349098009222\n",
      "Training loss: 0.5695056086953949   1050  0.5513364672660828\n",
      "Training loss: 0.5724747255444527   1051  0.5434296341503367\n",
      "Training loss: 0.5715546261738328   1052  0.5513819876839133\n",
      "Training loss: 0.5671154545510516   1053  0.5491120061453652\n",
      "Training loss: 0.5774769239565906   1054  0.5427760727265302\n",
      "Training loss: 0.5743223875761032   1055  0.5561758875846863\n",
      "Training loss: 0.5670834271346822   1056  0.5465959380654728\n",
      "Training loss: 0.580369184122366   1057  0.5604150593280792\n",
      "Training loss: 0.5709832634995965   1058  0.5405168322955861\n",
      "Training loss: 0.5690553293508642   1059  0.5531298886327183\n",
      "Training loss: 0.5727848542087218   1060  0.5460096439894508\n",
      "Training loss: 0.5850505920894006   1061  0.5597395616419175\n",
      "Training loss: 0.5617841539137504   1062  0.5407627277514514\n",
      "Training loss: 0.5660329408505383   1063  0.5563705616137561\n",
      "Training loss: 0.5670932364814422   1064  0.5696544612155241\n",
      "Training loss: 0.573802665752523   1065  0.5560238659381866\n",
      "Training loss: 0.5659616668434704   1066  0.5679071896216449\n",
      "Training loss: 0.57133940870271   1067  0.5444596041651333\n",
      "Training loss: 0.5705782253952587   1068  0.5523847569437588\n",
      "Training loss: 0.5661253784509266   1069  0.550683847245048\n",
      "Training loss: 0.5711551330545369   1070  0.5548581410856808\n",
      "Training loss: 0.5652649621753132   1071  0.5605936243253595\n",
      "Training loss: 0.5875226215404623   1072  0.5481482341009027\n",
      "Training loss: 0.5743803482721833   1073  0.5753455232171452\n",
      "Training loss: 0.5775631499641082   1074  0.5442401170730591\n",
      "Training loss: 0.5652096933301758   1075  0.5416286727961372\n",
      "Training loss: 0.578095464145436   1076  0.5457921887145323\n",
      "Training loss: 0.5657283023876303   1077  0.5547691040179309\n",
      "Training loss: 0.574685056858203   1078  0.5407139711520251\n",
      "Training loss: 0.5724345138844322   1079  0.552051857990377\n",
      "Training loss: 0.5599951621364144   1080  0.5485290657071507\n",
      "Training loss: 0.5642961951739648   1081  0.5561883537208333\n",
      "Training loss: 0.5743108584600336   1082  0.5422200472915873\n",
      "Training loss: 0.5713899845586103   1083  0.5408773229402655\n",
      "Training loss: 0.5605969350127613   1084  0.5420912609380835\n",
      "Training loss: 0.5682407424730413   1085  0.5537366271018982\n",
      "Training loss: 0.554374070290257   1086  0.5382228423567379\n",
      "Training loss: 0.5683619699933949   1087  0.5617249029524186\n",
      "Training loss: 0.5699944754733759   1088  0.5719554354162777\n",
      "Training loss: 0.5631194013883086   1089  0.5762351768858293\n",
      "Training loss: 0.5706929415464401   1090  0.5476689724361196\n",
      "Training loss: 0.5775167026063975   1091  0.5397797612582936\n",
      "Training loss: 0.5623898528078023   1092  0.5719608980066636\n",
      "Training loss: 0.574926512206302   1093  0.5662004334085128\n",
      "Training loss: 0.5628922472105307   1094  0.5603116592940163\n",
      "Training loss: 0.5793192938846701   1095  0.545066146289601\n",
      "Training loss: 0.5714273316895261   1096  0.5487491523518282\n",
      "Training loss: 0.5679962893619257   1097  0.5473092952195335\n",
      "Training loss: 0.5650478189482409   1098  0.5371017280746909\n",
      "Training loss: 0.5562677256324712   1099  0.5694881677627563\n",
      "Training loss: 0.570544660967939   1100  0.5553679957109339\n",
      "Training loss: 0.5722510082756772   1101  0.5522343902026906\n",
      "Training loss: 0.5674937477006632   1102  0.5512546791749842\n",
      "Training loss: 0.56878481454709   1103  0.5548613457118764\n",
      "Training loss: 0.5660025052287999   1104  0.5468691324486452\n",
      "Training loss: 0.5686846327255753   1105  0.5457946973688462\n",
      "Training loss: 0.5640052618349299   1106  0.5489328082870034\n",
      "Training loss: 0.567024468060802   1107  0.5488470897955053\n",
      "Training loss: 0.5639896511155016   1108  0.5452165393268361\n",
      "Training loss: 0.5601041211801416   1109  0.5577796872924355\n",
      "Training loss: 0.570825453628512   1110  0.5488604307174683\n",
      "Training loss: 0.5556035615942058   1111  0.5464447684147778\n",
      "Training loss: 0.5714612353373977   1112  0.5596639566561755\n",
      "Training loss: 0.5690764609505149   1113  0.5424764629672555\n",
      "Training loss: 0.5824168924899662   1114  0.5772083629580105\n",
      "Training loss: 0.5730786564595559   1115  0.5518150417243733\n",
      "Training loss: 0.5562651183675317   1116  0.5464940737275517\n",
      "Training loss: 0.5716349276549676   1117  0.5518412081634297\n",
      "Training loss: 0.5615695789456367   1118  0.5567440881448633\n",
      "Training loss: 0.565175159889109   1119  0.5467901264919954\n",
      "Training loss: 0.567911078386447   1120  0.5522082518128788\n",
      "Training loss: 0.5749218818895957   1121  0.5558889981578378\n",
      "Training loss: 0.5659038161530214   1122  0.5483847698744606\n",
      "Training loss: 0.5794589414316065   1123  0.5396591249634238\n",
      "Training loss: 0.5584140200825298   1124  0.5606759576236501\n",
      "Training loss: 0.5686240595053224   1125  0.566492638167213\n",
      "Training loss: 0.5747398344032905   1126  0.5557614827857298\n",
      "Training loss: 0.572101998855086   1127  0.5662899841280544\n",
      "Training loss: 0.5675059022272334   1128  0.5488521439187667\n",
      "Training loss: 0.5566951251205277   1129  0.5331775195458356\n",
      "Training loss: 0.5774305480367997   1130  0.5571476133430705\n",
      "Training loss: 0.5757220220916411   1131  0.5396235900766709\n",
      "Training loss: 0.581007921082132   1132  0.5411085854558384\n",
      "Training loss: 0.5610246246351915   1133  0.5424465221517226\n",
      "Training loss: 0.5655602462151471   1134  0.5498028432621676\n",
      "Training loss: 0.5711806065895978   1135  0.5496559318374185\n",
      "Training loss: 0.5670092662467676   1136  0.5341715654906105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5651240550419864   1137  0.5430360313724069\n",
      "Training loss: 0.5700223748298252   1138  0.5571224128498751\n",
      "Training loss: 0.5698948844390757   1139  0.5527047690223245\n",
      "Training loss: 0.5705860520110411   1140  0.5387052402776831\n",
      "Training loss: 0.5752159212442005   1141  0.5450368471005383\n",
      "Training loss: 0.5765909754178103   1142  0.5636202868293313\n",
      "Training loss: 0.5702707320451736   1143  0.5601849608561572\n",
      "Training loss: 0.5595709554412786   1144  0.5386994954417733\n",
      "Training loss: 0.5635244500111131   1145  0.5532715338117936\n",
      "Training loss: 0.5611906871199608   1146  0.547157899421804\n",
      "Training loss: 0.5631915523725397   1147  0.5618941328104805\n",
      "Training loss: 0.5728879987316973   1148  0.5494609054397134\n",
      "Training loss: 0.5648632465916521   1149  0.5434403156532961\n",
      "Training loss: 0.5531502234585145   1150  0.5722886516767389\n",
      "Training loss: 0.5672003894167787   1151  0.5484053737976972\n",
      "Training loss: 0.5713534447200158   1152  0.5538614497465246\n",
      "Training loss: 0.560328917030026   1153  0.5385141583049998\n",
      "Training loss: 0.5692302882671356   1154  0.5608956761219922\n",
      "Training loss: 0.5609257024000672   1155  0.5454551051644718\n",
      "Training loss: 0.5691804942839286   1156  0.5579474831328672\n",
      "Training loss: 0.5718357216785935   1157  0.5775286169613109\n",
      "Training loss: 0.5584596207036692   1158  0.5401760269613827\n",
      "Training loss: 0.5711946601376814   1159  0.5445163828485152\n",
      "Training loss: 0.572764223551049   1160  0.5495297295205733\n",
      "Training loss: 0.5670751192990471   1161  0.5360894325901481\n",
      "Training loss: 0.5676947428899652   1162  0.5647080365349265\n",
      "Training loss: 0.5536471635979765   1163  0.5496629371362574\n",
      "Training loss: 0.565126921324169   1164  0.5406237998429466\n",
      "Training loss: 0.5591836010708529   1165  0.5452600787667667\n",
      "Training loss: 0.5552811105461681   1166  0.5448884473127478\n",
      "Training loss: 0.5683340772109873   1167  0.5339730592334971\n",
      "Training loss: 0.5809315320323495   1168  0.5463254346567041\n",
      "Training loss: 0.5690492961336585   1169  0.5360152283135582\n",
      "Training loss: 0.5641943571322104   1170  0.5363818634958828\n",
      "Training loss: 0.5695765973890529   1171  0.5371040807050818\n",
      "Training loss: 0.5781471624093897   1172  0.5353767363464131\n",
      "Training loss: 0.5712083564961657   1173  0.54869645132738\n",
      "Training loss: 0.5742323814946062   1174  0.5350594134891734\n",
      "Training loss: 0.5667891918736345   1175  0.5485450853319729\n",
      "Training loss: 0.5652019495473188   1176  0.5621514723581427\n",
      "Training loss: 0.5761991971555878   1177  0.5551138011848226\n",
      "Training loss: 0.5756699951256022   1178  0.548894456204246\n",
      "Training loss: 0.5647299342295703   1179  0.5430267470724442\n",
      "Training loss: 0.5748867107664838   1180  0.5297977731508368\n",
      "Training loss: 0.5682394500164425   1181  0.5541480727055493\n",
      "Training loss: 0.5674708437393693   1182  0.5533976099070381\n",
      "Training loss: 0.5597376130959567   1183  0.533931074773564\n",
      "Training loss: 0.5655877055490718   1184  0.5409282112822813\n",
      "Training loss: 0.5814201139351901   1185  0.5803785148788901\n",
      "Training loss: 0.5615068858160692   1186  0.5563676532577065\n",
      "Training loss: 0.5615410234998254   1187  0.5421820475774652\n",
      "Training loss: 0.5654836489873774   1188  0.5419061534545001\n",
      "Training loss: 0.5700958235298886   1189  0.5504832197638119\n",
      "Training loss: 0.5712372410823318   1190  0.5474532106343437\n",
      "Training loss: 0.5739256936837646   1191  0.5472833707052118\n",
      "Training loss: 0.5586959500523174   1192  0.5490489654681262\n",
      "Training loss: 0.5567319748156211   1193  0.5572164952754974\n",
      "Training loss: 0.5744337439537048   1194  0.5390099006540635\n",
      "Training loss: 0.5759207755327225   1195  0.5426760789226083\n",
      "Training loss: 0.5769730368081261   1196  0.5455365706892574\n",
      "Training loss: 0.5679561329238555   1197  0.5957284937886631\n",
      "Training loss: 0.5613158389049417   1198  0.5472028869039872\n",
      "Training loss: 0.5773431816521812   1199  0.5298640131950378\n",
      "Training loss: 0.5627723156529314   1200  0.5540563674534068\n",
      "Training loss: 0.5536085595102871   1201  0.5484240651130676\n",
      "Training loss: 0.5653877398547005   1202  0.5513038670315462\n",
      "Training loss: 0.560767638770973   1203  0.5326599531313952\n",
      "Training loss: 0.562934580094674   1204  0.5425637416979846\n",
      "Training loss: 0.5666391551494598   1205  0.5403203105225283\n",
      "Training loss: 0.5729895624167779   1206  0.5381259515004999\n",
      "Training loss: 0.5696920130182715   1207  0.5552375965258655\n",
      "Training loss: 0.5600020955590641   1208  0.5558535667026744\n",
      "Training loss: 0.5770335916210624   1209  0.5429753976709703\n",
      "Training loss: 0.5678180013509357   1210  0.5562564853359672\n",
      "Training loss: 0.5712281610159313   1211  0.544075410155689\n",
      "Training loss: 0.5638710646068349   1212  0.5532616148976719\n",
      "Training loss: 0.5579106014441041   1213  0.5692538689164555\n",
      "Training loss: 0.5573210874024559   1214  0.5370835609295789\n",
      "Training loss: 0.5720199670861749   1215  0.5433423256172853\n",
      "Training loss: 0.5651438494815546   1216  0.5580483832780052\n",
      "Training loss: 0.5599451358704006   1217  0.5711618889780605\n",
      "Training loss: 0.5731837074546253   1218  0.5769029592766481\n",
      "Training loss: 0.5663930405588711   1219  0.5489025308805353\n",
      "Training loss: 0.5684353621566997   1220  0.5483059374725118\n",
      "Training loss: 0.5714090795201414   1221  0.5447285420754376\n",
      "Training loss: 0.5726587268359521   1222  0.5332010966889998\n",
      "Training loss: 0.5718075891627985   1223  0.5369671881198883\n",
      "Training loss: 0.567005980102455   1224  0.5894327163696289\n",
      "Training loss: 0.5714438491884399   1225  0.5658584622775807\n",
      "Training loss: 0.5690584217800814   1226  0.5899426253402934\n",
      "Training loss: 0.5749375482692438   1227  0.5467050093061784\n",
      "Training loss: 0.5803894251585007   1228  0.5315299595103544\n",
      "Training loss: 0.5722955390810966   1229  0.5310695381725535\n",
      "Training loss: 0.5588920690557536   1230  0.5558534808018628\n",
      "Training loss: 0.5723087625468478   1231  0.5584012725773979\n",
      "Training loss: 0.5758909035254928   1232  0.5425483338973102\n",
      "Training loss: 0.5734170491204542   1233  0.5383919845609104\n",
      "Training loss: 0.5670390672543469   1234  0.5427482881966759\n",
      "Training loss: 0.5665039139635423   1235  0.5609068590051988\n",
      "Training loss: 0.5746070168474141   1236  0.5559533641618841\n",
      "Training loss: 0.5684544203035972   1237  0.5412890700732961\n",
      "Training loss: 0.5621564383892452   1238  0.5316470300450045\n",
      "Training loss: 0.5770382740918327   1239  0.5545002239591935\n",
      "Training loss: 0.5533894423176261   1240  0.5377667020348942\n",
      "Training loss: 0.5710792462615406   1241  0.5293278396129608\n",
      "Training loss: 0.5744653589585248   1242  0.5579827305148629\n",
      "Training loss: 0.5633936818031704   1243  0.5504332868491902\n",
      "Training loss: 0.5678493485731237   1244  0.540141670142903\n",
      "Training loss: 0.5683851741692599   1245  0.5560061668648439\n",
      "Training loss: 0.5780009338084389   1246  0.5444787442684174\n",
      "Training loss: 0.5669980180614135   1247  0.5519707062665153\n",
      "Training loss: 0.5627501194967943   1248  0.543694348896251\n",
      "Training loss: 0.5554645412108478   1249  0.539582759141922\n",
      "Training loss: 0.5736744346864083   1250  0.5377967953681946\n",
      "Training loss: 0.5656892985982054   1251  0.5540010140222662\n",
      "Training loss: 0.5635859523625935   1252  0.5419838148004869\n",
      "Training loss: 0.5604503562345224   1253  0.5429536998271942\n",
      "Training loss: 0.5795917668763328   1254  0.5295130137135001\n",
      "Training loss: 0.5735205118270481   1255  0.5337574815048891\n",
      "Training loss: 0.5710958641241578   1256  0.5474538803100586\n",
      "Training loss: 0.5819791862193275   1257  0.537123078809065\n",
      "Training loss: 0.5756414458155632   1258  0.5495777726173401\n",
      "Training loss: 0.5714673088753924   1259  0.5488903960760902\n",
      "Training loss: 0.5655352460110888   1260  0.5268517136573792\n",
      "Training loss: 0.5663927916218253   1261  0.5425587910063127\n",
      "Training loss: 0.5650507663102711   1262  0.5455709555569817\n",
      "Training loss: 0.5638350121238652   1263  0.5528400487759534\n",
      "Training loss: 0.5682431021157432   1264  0.5491276961915633\n",
      "Training loss: 0.5717854929320952   1265  0.5355691278682035\n",
      "Training loss: 0.5815619484466665   1266  0.523687960470424\n",
      "Training loss: 0.5801673448261093   1267  0.5505698831642375\n",
      "Training loss: 0.5684905998847064   1268  0.5561589616186479\n",
      "Training loss: 0.5608480537638945   1269  0.5809223301270429\n",
      "Training loss: 0.5579017457716605   1270  0.5710308183641994\n",
      "Training loss: 0.557096940191353   1271  0.5403734042364008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.564115075942348   1272  0.5428538217264063\n",
      "Training loss: 0.5732219683773377   1273  0.5607126074678758\n",
      "Training loss: 0.5768508893602035   1274  0.5592284833683687\n",
      "Training loss: 0.5727949440479279   1275  0.5415915040408864\n",
      "Training loss: 0.5747634268858853   1276  0.5571679718354169\n",
      "Training loss: 0.5665828384020749   1277  0.5643285267493304\n",
      "Training loss: 0.5525654681465205   1278  0.5325451090055353\n",
      "Training loss: 0.5649439867805032   1279  0.5396867411978105\n",
      "Training loss: 0.5659923365010935   1280  0.5453457832336426\n",
      "Training loss: 0.5723565453115631   1281  0.5459292005090153\n",
      "Training loss: 0.5733939707279205   1282  0.5521724329275244\n",
      "Training loss: 0.5687978881246903   1283  0.5514445971040165\n",
      "Training loss: 0.5709361407686683   1284  0.5527408578816582\n",
      "Training loss: 0.5554664143744636   1285  0.535446764791713\n",
      "Training loss: 0.5671476520159665   1286  0.535730440826977\n",
      "Training loss: 0.5661980145994354   1287  0.5554964402142692\n",
      "Training loss: 0.5595154643935316   1288  0.529219958712073\n",
      "Training loss: 0.5612359410699677   1289  0.5413769921835732\n",
      "Training loss: 0.5649586153381011   1290  0.5319047307266909\n",
      "Training loss: 0.5657698393744581   1291  0.5818272513501784\n",
      "Training loss: 0.5686385920819115   1292  0.5427708906285903\n",
      "Training loss: 0.5651331977809176   1293  0.5867458143654991\n",
      "Training loss: 0.5746628979549688   1294  0.5322826662484337\n",
      "Training loss: 0.5649744004887693   1295  0.5571867034715765\n",
      "Training loss: 0.5624217671506545   1296  0.5669745750287\n",
      "Training loss: 0.5640359718133422   1297  0.5433147637283101\n",
      "Training loss: 0.5736799748504863   1298  0.5570615722852594\n",
      "Training loss: 0.5574101157048169   1299  0.5465613936676699\n",
      "Training loss: 0.5671815328738269   1300  0.5569132636575138\n",
      "Training loss: 0.5624206504401039   1301  0.5250257954877966\n",
      "Training loss: 0.561140648144133   1302  0.5408903149997487\n",
      "Training loss: 0.561258064911646   1303  0.5340326060267055\n",
      "Training loss: 0.5776027611949864   1304  0.5360948566128226\n",
      "Training loss: 0.5677122013533816   1305  0.5297058645416709\n",
      "Training loss: 0.5773570857503835   1306  0.5489777133745306\n",
      "Training loss: 0.5631648998926667   1307  0.5420431424589718\n",
      "Training loss: 0.5597011065658402   1308  0.5451794003739077\n",
      "Training loss: 0.5695678153458763   1309  0.5488541073658887\n",
      "Training loss: 0.5649811112705398   1310  0.5328879128484165\n",
      "Training loss: 0.5750004767495043   1311  0.5755925248650944\n",
      "Training loss: 0.5675965240773033   1312  0.531738141003777\n",
      "Training loss: 0.5709847277578186   1313  0.5439001654877382\n",
      "Training loss: 0.5740812385783476   1314  0.5503943439792184\n",
      "Training loss: 0.5502305775880814   1315  0.5439940866302041\n",
      "Training loss: 0.5589428697438801   1316  0.545785137835671\n",
      "Training loss: 0.5693002728854909   1317  0.5367980914957383\n",
      "Training loss: 0.5701985753634397   1318  0.5422220615779653\n",
      "Training loss: 0.5699127018451691   1319  0.5644080148023718\n",
      "Training loss: 0.5665095663245987   1320  0.5466421754921184\n",
      "Training loss: 0.5619397461414337   1321  0.5339922606945038\n",
      "Training loss: 0.5611270661739742   1322  0.5595714397290174\n",
      "Training loss: 0.5758476853370667   1323  0.5699258113608641\n",
      "Training loss: 0.5671987375792336   1324  0.5423325738486122\n",
      "Training loss: 0.5699794568559703   1325  0.5501048144172219\n",
      "Training loss: 0.5685862635864931   1326  0.5420831371756161\n",
      "Training loss: 0.5649896185187733   1327  0.551831932628856\n",
      "Training loss: 0.5642048843643245   1328  0.5384602231137893\n",
      "Training loss: 0.5724441750961191   1329  0.5527504121556002\n",
      "Training loss: 0.556437310488785   1330  0.5301868407165303\n",
      "Training loss: 0.5734832405167467   1331  0.5652806583572837\n",
      "Training loss: 0.5784006202045608   1332  0.5372715785222895\n",
      "Training loss: 0.5695118422017378   1333  0.5453575165832744\n",
      "Training loss: 0.5773560864960446   1334  0.5303690503625309\n",
      "Training loss: 0.5624067581751767   1335  0.5588681662783903\n",
      "Training loss: 0.5586096591809216   1336  0.5508292650475222\n",
      "Training loss: 0.5666864588856697   1337  0.541472843464683\n",
      "Training loss: 0.56370518882485   1338  0.5569481832139632\n",
      "Training loss: 0.5721567650051678   1339  0.5495037576731514\n",
      "Training loss: 0.5728961628149537   1340  0.5418612097992617\n",
      "Training loss: 0.5606222130796489   1341  0.5524553218308617\n",
      "Training loss: 0.5586975282605957   1342  0.5609040155130274\n",
      "Training loss: 0.5528701815535041   1343  0.5379339456558228\n",
      "Training loss: 0.5655496379031855   1344  0.5563449544065139\n",
      "Training loss: 0.571247414631002   1345  0.5303501118631924\n",
      "Training loss: 0.5702121012351092   1346  0.5581240548807032\n",
      "Training loss: 0.5742408536812839   1347  0.5561421829111436\n",
      "Training loss: 0.5594168393050923   1348  0.5853265120702631\n",
      "Training loss: 0.5634852284017731   1349  0.590601012987249\n",
      "Training loss: 0.5584641230456969   1350  0.5567761536906747\n",
      "Training loss: 0.5595657615100637   1351  0.540255969061571\n",
      "Training loss: 0.5697122886776924   1352  0.5440117541481467\n",
      "Training loss: 0.5525769876206622   1353  0.5370959218810586\n",
      "Training loss: 0.5668640710851726   1354  0.5697159732089323\n",
      "Training loss: 0.5724286152159467   1355  0.5424973666667938\n",
      "Training loss: 0.5584071269806694   1356  0.5669893657459932\n",
      "Training loss: 0.5595338848583838   1357  0.5394779882010292\n",
      "Training loss: 0.5812901179580128   1358  0.5311602424172794\n",
      "Training loss: 0.5704006526400062   1359  0.5296617150306702\n",
      "Training loss: 0.5693469700567863   1360  0.5443527716047624\n",
      "Training loss: 0.5648909890476395   1361  0.5536986010916093\n",
      "Training loss: 0.5567714747260598   1362  0.5523693070692175\n",
      "Training loss: 0.5665512400514939   1363  0.5417301479507896\n",
      "Training loss: 0.5672835948712686   1364  0.5455711466424605\n",
      "Training loss: 0.5740884958821184   1365  0.5501076842055601\n",
      "Training loss: 0.5766040446127162   1366  0.5411635619752547\n",
      "Training loss: 0.567619524019606   1367  0.5452313580933739\n",
      "Training loss: 0.5648230866474264   1368  0.5335913002490997\n",
      "Training loss: 0.5633793990401661   1369  0.5232541263103485\n",
      "Training loss: 0.5611378160469672   1370  0.5405568655799416\n",
      "Training loss: 0.5715693533420563   1371  0.5532820259823519\n",
      "Training loss: 0.5658962178756209   1372  0.5433118255699382\n",
      "Training loss: 0.5741394048227983   1373  0.5430633916574366\n",
      "Training loss: 0.5585069003350595   1374  0.5307493560454425\n",
      "Training loss: 0.5668185199884808   1375  0.5481129832127515\n",
      "Training loss: 0.5806448117775076   1376  0.5564199370496413\n",
      "Training loss: 0.5651013951967744   1377  0.5322574657552382\n",
      "Training loss: 0.5748008726274266   1378  0.5384294636109296\n",
      "Training loss: 0.5653151392060167   1379  0.532304493819966\n",
      "Training loss: 0.5734806556035491   1380  0.5429399416727179\n",
      "Training loss: 0.5659358177114936   1381  0.5304865854627946\n",
      "Training loss: 0.5701175147996229   1382  0.5724366079358494\n",
      "Training loss: 0.5631126713226823   1383  0.5262200727182276\n",
      "Training loss: 0.5685560229946586   1384  0.5405303327476277\n",
      "Training loss: 0.5688520569135161   1385  0.5484701833304237\n",
      "Training loss: 0.5610754183110069   1386  0.5337617414839128\n",
      "Training loss: 0.5773475669762668   1387  0.5805439598420087\n",
      "Training loss: 0.5578781221719349   1388  0.5467668789274552\n",
      "Training loss: 0.575447690399254   1389  0.5341839983182795\n",
      "Training loss: 0.556717133259072   1390  0.531756777973736\n",
      "Training loss: 0.5651392827139181   1391  0.5717502359081718\n",
      "Training loss: 0.5592526131693054   1392  0.5600813767489266\n",
      "Training loss: 0.5725316080976935   1393  0.5346712557708516\n",
      "Training loss: 0.5587193435605835   1394  0.5422637795700747\n",
      "Training loss: 0.5588987408315435   1395  0.5257943588144639\n",
      "Training loss: 0.5675783744629692   1396  0.5416894835584304\n",
      "Training loss: 0.5607892594793263   1397  0.5241143808645361\n",
      "Training loss: 0.5755275676355642   1398  0.5303253619109883\n",
      "Training loss: 0.5636906080386218   1399  0.5446374574128319\n",
      "Training loss: 0.5676142875762547   1400  0.5503690786221448\n",
      "Training loss: 0.570143820170094   1401  0.5264952340546776\n",
      "Training loss: 0.5604445456581957   1402  0.5538388367961434\n",
      "Training loss: 0.5549647098078447   1403  0.5336275381200454\n",
      "Training loss: 0.5837169999585432   1404  0.5627099258058211\n",
      "Training loss: 0.5619053240208065   1405  0.5322163350441876\n",
      "Training loss: 0.5512531938798287   1406  0.5522139528218437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5514005930984721   1407  0.5530444664113662\n",
      "Training loss: 0.5631658807396889   1408  0.5328707063899321\n",
      "Training loss: 0.5801025686018607   1409  0.5359170805005466\n",
      "Training loss: 0.5689249152646345   1410  0.5535138161743388\n",
      "Training loss: 0.5593134717029684   1411  0.5438315359985127\n",
      "Training loss: 0.5697425840532079   1412  0.5420027462875142\n",
      "Training loss: 0.5790421195766505   1413  0.5328402782187742\n",
      "Training loss: 0.5765010420890415   1414  0.5358223757323097\n",
      "Training loss: 0.5573751957100981   1415  0.5389992138918709\n",
      "Training loss: 0.5731712781331119   1416  0.5276897199013654\n",
      "Training loss: 0.5757173413739485   1417  0.5417546331882477\n",
      "Training loss: 0.5682214239940924   1418  0.5629317725405973\n",
      "Training loss: 0.5698680040590903   1419  0.5711942385224735\n",
      "Training loss: 0.5728972856612766   1420  0.5364823183592629\n",
      "Training loss: 0.5797422690426602   1421  0.5491470957503599\n",
      "Training loss: 0.5575906476553749   1422  0.5872016303679523\n",
      "Training loss: 0.5669265402590528   1423  0.5359586985672221\n",
      "Training loss: 0.5676630485583755   1424  0.5396787717061884\n",
      "Training loss: 0.5628458638401592   1425  0.5528344459393445\n",
      "Training loss: 0.5863150906036881   1426  0.5344228762037614\n",
      "Training loss: 0.5643110152553109   1427  0.5347959206384771\n",
      "Training loss: 0.5679956757847   1428  0.5441199435907251\n",
      "Training loss: 0.5817496487322975   1429  0.5337813443997327\n",
      "Training loss: 0.573167218004956   1430  0.5469032035154455\n",
      "Training loss: 0.5709733726347194   1431  0.5354153461316052\n",
      "Training loss: 0.5698599039631731   1432  0.5364419099162606\n",
      "Training loss: 0.5407895835883477   1433  0.5553401357987348\n",
      "Training loss: 0.5593850520603797   1434  0.5351039591957542\n",
      "Training loss: 0.5605058613068917   1435  0.5331313014030457\n",
      "Training loss: 0.5723987397025613   1436  0.5468316306086147\n",
      "Training loss: 0.5633193657678717   1437  0.549199104309082\n",
      "Training loss: 0.5715572781422559   1438  0.5249423261950997\n",
      "Training loss: 0.579329074743916   1439  0.5329653007142684\n",
      "Training loss: 0.5697125617195579   1440  0.5445024493862601\n",
      "Training loss: 0.5714168780866791   1441  0.5588818578159108\n",
      "Training loss: 0.5678085689159   1442  0.5267447303323185\n",
      "Training loss: 0.5723017243778005   1443  0.5702923623954549\n",
      "Training loss: 0.5655553003444391   1444  0.5525113589623395\n",
      "Training loss: 0.5665781340178322   1445  0.5386041122324327\n",
      "Training loss: 0.5585757980451864   1446  0.5440768652102527\n",
      "Training loss: 0.5726444979800898   1447  0.534918965662227\n",
      "Training loss: 0.5671938627081758   1448  0.5403312900487114\n",
      "Training loss: 0.5681813363643253   1449  0.5303120630628922\n",
      "Training loss: 0.5654062948682729   1450  0.5345255189082202\n",
      "Training loss: 0.5624818595893243   1451  0.5853869704639211\n",
      "Training loss: 0.5586117720779251   1452  0.5331896981772255\n",
      "Training loss: 0.5755576618454036   1453  0.5320568540517021\n",
      "Training loss: 0.5656629603575257   1454  0.5300076744135689\n",
      "Training loss: 0.5645466109409052   1455  0.5302369471858529\n",
      "Training loss: 0.5650584487354054   1456  0.5452704499749577\n",
      "Training loss: 0.5605523472323137   1457  0.5473310877295101\n",
      "Training loss: 0.5646881570710856   1458  0.5763787048704484\n",
      "Training loss: 0.5824910165632472   1459  0.546472479315365\n",
      "Training loss: 0.5740090350017828   1460  0.5673535448663375\n",
      "Training loss: 0.5752248176757027   1461  0.5368060171604156\n",
      "Training loss: 0.5635388909893877   1462  0.5315509701476377\n",
      "Training loss: 0.5687221937319812   1463  0.5403949004762313\n",
      "Training loss: 0.5614329690442366   1464  0.5416579877629\n",
      "Training loss: 0.565707278163994   1465  0.534512411145603\n",
      "Training loss: 0.5754422641852323   1466  0.5350989664302153\n",
      "Training loss: 0.5590694261824384   1467  0.5308588971109951\n",
      "Training loss: 0.5747509585583911   1468  0.5510857122785905\n",
      "Training loss: 0.5664821385460741   1469  0.5591986354659585\n",
      "Training loss: 0.5580631481374011   1470  0.5221613382591921\n",
      "Training loss: 0.5606823664377717   1471  0.5511801803813261\n",
      "Training loss: 0.5608793444493237   1472  0.5432682773646187\n",
      "Training loss: 0.5555099253268803   1473  0.5283088824328255\n",
      "Training loss: 0.566793346667991   1474  0.5466498469605165\n",
      "Training loss: 0.562839537858963   1475  0.5296083566020516\n",
      "Training loss: 0.5614640677676481   1476  0.5299718800713035\n",
      "Training loss: 0.5658400440040756   1477  0.5468770475948558\n",
      "Training loss: 0.5534335269647486   1478  0.5874529568588033\n",
      "Training loss: 0.5569971926948604   1479  0.5520391253864064\n",
      "Training loss: 0.5634010495508418   1480  0.5538085716612199\n",
      "Training loss: 0.5636745980557274   1481  0.5431579737102284\n",
      "Training loss: 0.5708275682785932   1482  0.5394215057877934\n",
      "Training loss: 0.5657102632171968   1483  0.5597370144198922\n",
      "Training loss: 0.5752721336834571   1484  0.527984552523669\n",
      "Training loss: 0.5560273238841225   1485  0.5408864301793715\n",
      "Training loss: 0.5680225500289131   1486  0.540365054326899\n",
      "Training loss: 0.5623423768316999   1487  0.5532131913830253\n",
      "Training loss: 0.5697894714334432   1488  0.5329854803926805\n",
      "Training loss: 0.5655367558493334   1489  0.553771609769148\n",
      "Training loss: 0.5671709517345709   1490  0.526745328131844\n",
      "Training loss: 0.5791938567862791   1491  0.5252809156389797\n",
      "Training loss: 0.565691116539871   1492  0.5357078033335069\n",
      "Training loss: 0.5628421609016026   1493  0.542562418124255\n",
      "Training loss: 0.5787166584940517   1494  0.5351795466507182\n",
      "Training loss: 0.5714855106437907   1495  0.5316118370084202\n",
      "Training loss: 0.5694383825449383   1496  0.5247278669301201\n",
      "Training loss: 0.559366824434084   1497  0.5256023214143866\n",
      "Training loss: 0.5671691780581194   1498  0.5288577097303727\n",
      "Training loss: 0.5637464965967571   1499  0.548436396262225\n"
     ]
    }
   ],
   "source": [
    "for i in range(RunForNumEpochs):\n",
    "    model.train()\n",
    "    running_loss=0\n",
    "    for data, labels in my_trainloader:\n",
    "        optimizer.zero_grad()    \n",
    "        result  = model(data)\n",
    "        loss =criterion(result,labels)\n",
    "        #print(result)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    model.eval();\n",
    "    TestLoss=0\n",
    "    for data, labels in my_testloader:\n",
    "        result = model(data)  \n",
    "        loss = criterion(result,labels)\n",
    "        TestLoss += loss.item()\n",
    "    LossOverEpoch.append(running_loss/len(my_trainloader))\n",
    "    EvalLoss.append(TestLoss/len(my_testloader))\n",
    "    print(str(f\"Training loss: {running_loss/len(my_trainloader)}\" +\"   \"+ str(Counter))+\"  \"+str(TestLoss/len(my_testloader)))\n",
    "    Counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(LossOverEpoch,label='Training Loss')\n",
    "plt.plot(EvalLoss,label='Test Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101  29]\n",
      " [ 19 121]]\n",
      "Accuracy of TimbralData : 77 %\n",
      "Accuracy of PitchData : 86 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ImageCalc/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(len(classes)))\n",
    "class_total = list(0. for i in range(len(classes)))\n",
    "model.eval()\n",
    "allLabels=[]\n",
    "allPrediction=[]\n",
    "with torch.no_grad():\n",
    "    for data in my_testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        if (c.dim()==0):\n",
    "            continue\n",
    "        for i in range(my_testloader.batch_size):\n",
    "            if(len(labels)<=i):\n",
    "                continue;\n",
    "            label = labels[i]\n",
    "            allLabels.append(labels[i].to('cpu').numpy())\n",
    "            allPrediction.append(predicted[i].to('cpu').numpy())\n",
    "            if(my_testloader.batch_size>1):\n",
    "                class_correct[label] += c[i].item()\n",
    "            else:\n",
    "                class_correct[label] += c.item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "print(confusion_matrix(allLabels, allPrediction))\n",
    "for i in range(len(classes)):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (ImageCalc)",
   "language": "python",
   "name": "imagecalc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
